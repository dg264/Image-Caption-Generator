{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALr2XZVHEY21"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import os\n",
        "from os import listdir\n",
        "from pickle import dump\n",
        "import keras\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIuah5GNEehy"
      },
      "source": [
        "def extract_features(directory):\n",
        "    #loading vgg model\n",
        "    model = VGG16()\n",
        "    #restructure model by removing the prediction layer as we don't need predictions we need encodings\n",
        "    model.layers.pop()\n",
        "    #initialising new model\n",
        "    model = Model(inputs = model.inputs , outputs = model.layers[-1].output)\n",
        "    print(model.summary())\n",
        "    #dictionary to save encodings of each image\n",
        "    features = dict()\n",
        "    #directory is path address of image folder\n",
        "    for name in listdir(directory):\n",
        "        #address of each image\n",
        "        filename = directory + '/' + name\n",
        "        #loading image tensor by changing dimensions\n",
        "        image = load_img(filename , target_size=(224 , 224))\n",
        "        #tranforming image from tensor to numpy array\n",
        "        image = img_to_array(image)\n",
        "        #reshaping image as per requirements of vgg network\n",
        "        image = image.reshape((1 , image.shape[0] , image.shape[1] , image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        #predicting encodings of image\n",
        "        feature = model.predict(image , verbose = 0)\n",
        "        image_id = name.split('.')[0]\n",
        "        features[image_id] = feature\n",
        "        print(name)\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toyMfVpkEpou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0368b0bc-62e1-4782-ed08-cde23a9692d0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8yyhrrOKCGi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d1bde10f-2224-402c-f170-02a9357ffcc1"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryeJPCfPLfn-"
      },
      "source": [
        "filename = 'Image_Caption_Project/features.pkl'\n",
        "infile = open(filename,'rb')\n",
        "features = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhPboQ3IRVIG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f1794f3-d0f4-46bd-8bbb-6b0ea7e98a92"
      },
      "source": [
        "features['862177617_c2c0581075'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 4096)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFiBQqLzRcVh"
      },
      "source": [
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_17m3oaRSux_"
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "    #open file as read only\n",
        "    file = open(filename , 'r')\n",
        "    #read all text\n",
        "    text = file.read()\n",
        "    #close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQKIx-hpSy97"
      },
      "source": [
        "#loading descriptions of each image from doc file we created from load_doc function\n",
        "def load_descriptions(doc):\n",
        "    mapping = dict()\n",
        "    #process lines\n",
        "    for line in doc.split('\\n'):\n",
        "        #split by white spaces\n",
        "        tokens = line.split()\n",
        "        if(len(line) < 2):\n",
        "            continue\n",
        "        #first token will be image id and remaining will be decription\n",
        "        image_id , image_desc = tokens[0] , tokens[1:]\n",
        "        #remove file name from image id\n",
        "        image_id = image_id.split('.')[0]\n",
        "        #convert description token back to strings\n",
        "        image_desc = ' '.join(image_desc)\n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id] = list()\n",
        "        #storing descriptions according to image\n",
        "        mapping[image_id].append(image_desc)\n",
        "    return mapping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN-djgPVS2PU"
      },
      "source": [
        "#function to clean all punctuations(. , ?)\n",
        "def clean_descriptions(descriptions):\n",
        "    #loading puctuations\n",
        "    punctuations = string.punctuation\n",
        "    for key , desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            #splliting description in list of words\n",
        "            desc = desc.split()\n",
        "            #removing punctuations and capital words and 1 letter words from the description\n",
        "            desc = [word.lower() for word in desc if (word.lower() not in punctuations) and (len(word.lower()) > 1)]\n",
        "            #removing numbers from description\n",
        "            desc = [word for word in desc if word.isalpha()]\n",
        "            #making list to string again and storing the description\n",
        "            desc_list[i] = ' '.join(desc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN3V1ahiTEva"
      },
      "source": [
        "#making vocabulary of words\n",
        "def create_vocabulary(descriptions):\n",
        "    all_desc = set()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "    return all_desc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPWJb2mNTLdf"
      },
      "source": [
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr8k.token.txt'\n",
        "#load descriptions\n",
        "doc = load_doc(filename)\n",
        "descriptions = load_descriptions(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTDlqT9rVX6k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4d5081d6-a054-464c-abbc-5a78bd8a815f"
      },
      "source": [
        "len(descriptions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFGxcdPRVgUe"
      },
      "source": [
        "clean_descriptions(descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpExGBolVzUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73142c62-e884-4ad6-954a-3055b8c5a644"
      },
      "source": [
        "len(descriptions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNYoj9AiV_27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "3c8e0901-ef35-4613-b305-2ba1afd94216"
      },
      "source": [
        "descriptions['667626_18933d713e']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['girl is stretched out in shallow water',\n",
              " 'girl wearing red and bikini is laying on her back in shallow water',\n",
              " 'little girl in red swimsuit is laying on her back in shallow water',\n",
              " 'young girl is lying in the sand while ocean water is surrounding her',\n",
              " 'girl wearing bikini lying on her back in shallow pool of clear blue water']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We-7gHVuWJSF"
      },
      "source": [
        "vocabulary = create_vocabulary(descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TmQe1g7XBbH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96a6f548-55b9-42a4-e49d-07a03fb9d748"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8357"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ugjeHNuXEFb"
      },
      "source": [
        "dump(vocabulary , open('Image_Caption_Project/vocabulary.pkl' , 'wb'))\n",
        "dump(descriptions , open('Image_Caption_Project/descriptions.pkl' , 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy5KyaNaYj1A"
      },
      "source": [
        "infile = open('Image_Caption_Project/vocabulary.pkl' , 'rb')\n",
        "vocabulary = pickle.load(infile)\n",
        "infile.close()\n",
        "infile = open('Image_Caption_Project/descriptions.pkl' , 'rb')\n",
        "descriptions = pickle.load(infile)\n",
        "infile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9byBoEUNXZBL"
      },
      "source": [
        "#loading pictures for train/dev set\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  for line in doc.split('\\n'):\n",
        "    if(len(line) < 1):\n",
        "      continue\n",
        "    #get image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB1eUwwRaPF5"
      },
      "source": [
        "#making training/testing dataset\n",
        "def load_clean_descriptions(filename , dataset):\n",
        "  infile = open(filename , 'rb')\n",
        "  descriptions = pickle.load(infile)\n",
        "  infile.close()\n",
        "  newdescriptions = dict()\n",
        "  for image_id in dataset:\n",
        "    if image_id in descriptions:\n",
        "      newdescriptions[image_id] = list()\n",
        "      for desc in descriptions[image_id]:\n",
        "        #wrap descriptions in tokens\n",
        "        newdescriptions[image_id].append('startseq ' + desc + ' endseq')\n",
        "  return newdescriptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqZbeBPnj8bU"
      },
      "source": [
        "def load_photo_features(filename , dataset):\n",
        "  all_features = pickle.load(open(filename , 'rb'))\n",
        "  features = dict()\n",
        "  for image_id in dataset:\n",
        "    if image_id in all_features:\n",
        "      features[image_id] = all_features[image_id]\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs6QNOlkm9LM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0bd4852a-8ecd-4fb8-be4a-39e5c6d40cf3"
      },
      "source": [
        "#loading traindata\n",
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print(len(train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAddCVx_n4j4"
      },
      "source": [
        "#load descriptions of train images\n",
        "train_descriptions = load_clean_descriptions('Image_Caption_Project/descriptions.pkl' , train)\n",
        "#load photo features\n",
        "train_features = load_photo_features('Image_Caption_Project/features.pkl' , train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmJp_-QLpZ4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b710877d-8ba1-4c36-88cb-1b019e0d43bc"
      },
      "source": [
        "print(train_descriptions['2513260012_03d33305cf'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['startseq black dog is running after white dog in the snow endseq', 'startseq black dog chasing brown dog through snow endseq', 'startseq two dogs chase each other across the snowy ground endseq', 'startseq two dogs play together in the snow endseq', 'startseq two dogs running through low lying body of water endseq']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Bl22jppjrG"
      },
      "source": [
        "import tensorflow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEl50NOHD1mi"
      },
      "source": [
        "#convert dictionary of discriptions to list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions:\n",
        "    for desc in descriptions[key]:\n",
        "      all_desc.append(desc)\n",
        "  return all_desc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7xCUB8HEh0B"
      },
      "source": [
        "#fit a tokenizer which maps all the words in descriptions to indices\n",
        "def create_tokenizer(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxtGJAEHFMm5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "240ea4a8-b64e-44f1-f5f9-1f100ade870f"
      },
      "source": [
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACKSwwTZHg6I"
      },
      "source": [
        "#returns length of description with maximum words\n",
        "def Max_length(descriptions):\n",
        "  max_len = 0\n",
        "  for key in descriptions:\n",
        "    for desc in descriptions[key]:\n",
        "      max_len = max(len(desc.split()) , max_len)\n",
        "  return max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuEBcr29Mw-Q"
      },
      "source": [
        "def create_sequences(tokenizer , max_length , descriptions , photos , vocab_size):\n",
        "  X1 , X2 , y = list() , list() , list()\n",
        "  for key , desc_list in descriptions.items():\n",
        "    for desc in desc_list:\n",
        "      seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "      for i in range(1 , len(seq)):\n",
        "        in_seq , out_seq = seq[:i] , seq[i]\n",
        "        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "        X1.append(photos[key][0])\n",
        "        X2.append(in_seq)\n",
        "        y.append(out_seq)\n",
        "  return np.array(X1) , np.array(X2) , np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGzvDhEMC_Cq"
      },
      "source": [
        "def define_model(vocab_size , max_len):\n",
        "  #processing input image\n",
        "  inputs1 = Input(shape = (4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(256 , activation='relu')(fe1)\n",
        "\n",
        "  #processing descriptions\n",
        "  inputs2 = Input(shape = (max_len,))\n",
        "  se1 = Embedding(vocab_size , 256 , mask_zero = True)(inputs2)\n",
        "  se2 = Dropout(0.5)(se1)\n",
        "  se3 = LSTM(256)(se2)\n",
        "\n",
        "  #decoder model\n",
        "  decoder1 = add([fe2 , se3])\n",
        "  decoder2 = Dense(256 , activation='relu')(decoder1)\n",
        "  outputs = Dense(vocab_size , activation='softmax')(decoder2)\n",
        "\n",
        "  #initialize the model\n",
        "  model = Model(inputs=[inputs1 , inputs2] , outputs = outputs)\n",
        "  #compiling model\n",
        "  model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam')\n",
        "  #printing summary of model\n",
        "  print(model.summary())\n",
        "  plot_model(model, to_file='model.png', show_shapes=True)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvb6hMV9OPlW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "399047d3-9b20-43ff-b851-9e2c2cb4656c"
      },
      "source": [
        "#training model\n",
        "\n",
        "#load train dataset\n",
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train_dataset = load_set(filename)\n",
        "print(\"len train dataset:\" , end = \" \")\n",
        "print(len(train_dataset))\n",
        "train_features = load_photo_features('Image_Caption_Project/features.pkl' , train_dataset)\n",
        "train_descriptions = load_clean_descriptions('Image_Caption_Project/descriptions.pkl' , train_dataset)\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"vocabsize:\" , end = \" \")\n",
        "print(vocab_size)\n",
        "max_len = Max_length(train_descriptions)\n",
        "print(\"max_length:\" , end = \" \")\n",
        "print(max_len)\n",
        "X1train , X2train , Ytrain = create_sequences(tokenizer , max_len , train_descriptions , train_features , vocab_size)\n",
        "\n",
        "#load dev dataset\n",
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "dev_dataset = load_set(filename)\n",
        "print(\"len dev dataset:\" , end = \" \")\n",
        "print(len(dev_dataset))\n",
        "dev_features = load_photo_features('Image_Caption_Project/features.pkl' , train_dataset)\n",
        "dev_descriptions = load_clean_descriptions('Image_Caption_Project/descriptions.pkl' , train_dataset)\n",
        "X1dev , X2dev , Ydev = create_sequences(tokenizer , max_len , dev_descriptions , dev_features , vocab_size)\n",
        "#####this method is giving me memory limit exceeded error#########"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len train dataset: 6000\n",
            "vocabsize: 7266\n",
            "max_length: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeiDWJJ-fxk2"
      },
      "source": [
        "def create_sequences2(tokenizer , desc_list , photo , max_len , vocab_size):\n",
        "  X1 , X2 , y = list() , list() , list()\n",
        "  for desc in desc_list:\n",
        "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "    for i in range(1 , len(seq)):\n",
        "      in_seq , out_seq = seq[:i] , seq[i]\n",
        "      in_seq = pad_sequences([in_seq], maxlen=max_len)[0]\n",
        "      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "      X1.append(photo)\n",
        "      X2.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "  return np.array(X1) , np.array(X2) , np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXSUIxElWnmU"
      },
      "source": [
        "#at each step data generator gives out dataset of a single image\n",
        "def data_generator(descriptions, photos, tokenizer, max_len, vocab_size):\n",
        "  #loop for all images\n",
        "  while True:\n",
        "    for key, desc_list in descriptions.items():\n",
        "      photo = photos[key][0]\n",
        "      in_img, in_seq, out_word = create_sequences2(tokenizer, desc_list, photo, max_len , vocab_size)\n",
        "      yield ((in_img, in_seq), out_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krd08edNcUIo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "e866b171-efdd-4dfe-f69e-25477b11d0d7"
      },
      "source": [
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train_dataset = load_set(filename)\n",
        "print(\"len train dataset:\" , end = \" \")\n",
        "print(len(train_dataset))\n",
        "train_features = load_photo_features('Image_Caption_Project/features.pkl' , train_dataset)\n",
        "train_descriptions = load_clean_descriptions('Image_Caption_Project/descriptions.pkl' , train_dataset)\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"vocabsize:\" , end = \" \")\n",
        "print(vocab_size)\n",
        "max_len = Max_length(train_descriptions)\n",
        "print(\"max_length:\" , end = \" \")\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len train dataset: 6000\n",
            "vocabsize: 7266\n",
            "max_length: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ea7lfX_fucC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "6aec8780-625b-413a-f366-82a811741e5c"
      },
      "source": [
        "model = define_model(vocab_size , max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 33, 256)      1860096     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 33, 256)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7266)         1867362     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,367,394\n",
            "Trainable params: 5,367,394\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnEMD1SlqRHv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91e74ea4-d33c-4247-f4ab-69801e6d6708"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_sRkNB9iYqw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "4c83726f-5127-4172-a3dd-a9c725f62a50"
      },
      "source": [
        "epochs = 20\n",
        "steps = len(train_descriptions)\n",
        "#as at each step data generator generates data of a single image thats why we need steps_per_epoch = 6000 as no. of images are 6000 in training dataset\n",
        "for i in range(epochs):\n",
        "  generator = data_generator(train_descriptions, train_features, tokenizer, max_len, vocab_size)\n",
        "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "  filename = 'Image_Caption_Project/'\n",
        "  model.save(filename + 'model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-39-05999d7b7dcc>:5: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "6000/6000 [==============================] - 598s 100ms/step - loss: 4.6611\n",
            "6000/6000 [==============================] - 585s 97ms/step - loss: 3.8680\n",
            "6000/6000 [==============================] - 591s 99ms/step - loss: 3.6152\n",
            "6000/6000 [==============================] - 567s 94ms/step - loss: 3.4799\n",
            "6000/6000 [==============================] - 582s 97ms/step - loss: 3.3666\n",
            "6000/6000 [==============================] - 577s 96ms/step - loss: 3.2953\n",
            "6000/6000 [==============================] - 565s 94ms/step - loss: 3.2414\n",
            "6000/6000 [==============================] - 579s 96ms/step - loss: 3.2051\n",
            "6000/6000 [==============================] - 579s 96ms/step - loss: 3.1749\n",
            "6000/6000 [==============================] - 569s 95ms/step - loss: 3.1522\n",
            "6000/6000 [==============================] - 567s 95ms/step - loss: 3.1269\n",
            "6000/6000 [==============================] - 566s 94ms/step - loss: 3.1109\n",
            "6000/6000 [==============================] - 563s 94ms/step - loss: 3.1043\n",
            "6000/6000 [==============================] - 563s 94ms/step - loss: 3.0883\n",
            "6000/6000 [==============================] - 561s 93ms/step - loss: 3.0777\n",
            "6000/6000 [==============================] - 563s 94ms/step - loss: 3.0682\n",
            "6000/6000 [==============================] - 562s 94ms/step - loss: 3.0679\n",
            "6000/6000 [==============================] - 568s 95ms/step - loss: 3.0582\n",
            "6000/6000 [==============================] - 563s 94ms/step - loss: 3.0563\n",
            "6000/6000 [==============================] - 563s 94ms/step - loss: 3.0546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0N6t5nrqb0R"
      },
      "source": [
        "def idx_to_word(tokenizer , idx):\n",
        "  if idx in tokenizer.index_word:\n",
        "    return tokenizer.index_word[idx]\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQimMpxKm8Ef"
      },
      "source": [
        "def generate_descriptions(model , tokenizer , photo , maxlen):\n",
        "  in_text = 'startseq'\n",
        "  for i in range(maxlen):\n",
        "    sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "    sequence = pad_sequences([sequence] , maxlen=max_len)\n",
        "    y_hat = model.predict([photo , sequence] , verbose = 0)\n",
        "    y_hat = np.argmax(y_hat)\n",
        "    word = idx_to_word(tokenizer , y_hat)\n",
        "    if word is None:\n",
        "      break\n",
        "    in_text += ' ' + word\n",
        "    if word == 'endseq':\n",
        "      break\n",
        "  return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsAuOBuxosRM"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKVTROrvo-mN"
      },
      "source": [
        "def evaluate_model(model , descriptions , photos , tokenizer , max_len):\n",
        "  actual , predicted = list() , list()\n",
        "  for key , desc_list in descriptions.items():\n",
        "    yhat = generate_descriptions(model , tokenizer , photos[key] , max_len)\n",
        "    references = [d.split() for d in desc_list]\n",
        "    actual.append(references)\n",
        "    predicted.append(yhat.split())\n",
        "  #calculating blue score\n",
        "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZgHd1U1tLwK"
      },
      "source": [
        "from keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihEgrcpmrXh4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "dff0cd39-8302-4981-88a5-6540b70ef739"
      },
      "source": [
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "train_descriptions = load_clean_descriptions('Image_Caption_Project/descriptions.pkl', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "max_len = Max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_len)\n",
        "\n",
        "filename = 'Image_Caption_Project/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "test_descriptions = load_clean_descriptions('Image_Caption_Project/descriptions.pkl', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "test_features = load_photo_features('Image_Caption_Project/features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        " \n",
        "# load the model\n",
        "for i in range(20):\n",
        "  filename = 'Image_Caption_Project/model_' + str(i) + '.h5'\n",
        "  model = load_model(filename)\n",
        "  print('model'+str(i)+' :')\n",
        "  evaluate_model(model, test_descriptions, test_features, tokenizer, max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Vocabulary Size: 7266\n",
            "Description Length: 33\n",
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n",
            "model0 :\n",
            "BLEU-1: 0.489197\n",
            "BLEU-2: 0.250274\n",
            "BLEU-3: 0.161983\n",
            "BLEU-4: 0.063646\n",
            "model1 :\n",
            "BLEU-1: 0.522877\n",
            "BLEU-2: 0.265724\n",
            "BLEU-3: 0.173097\n",
            "BLEU-4: 0.074125\n",
            "model2 :\n",
            "BLEU-1: 0.525655\n",
            "BLEU-2: 0.267379\n",
            "BLEU-3: 0.172641\n",
            "BLEU-4: 0.070918\n",
            "model3 :\n",
            "BLEU-1: 0.508897\n",
            "BLEU-2: 0.254281\n",
            "BLEU-3: 0.163904\n",
            "BLEU-4: 0.067186\n",
            "model4 :\n",
            "BLEU-1: 0.528120\n",
            "BLEU-2: 0.262177\n",
            "BLEU-3: 0.166713\n",
            "BLEU-4: 0.068417\n",
            "model5 :\n",
            "BLEU-1: 0.515237\n",
            "BLEU-2: 0.255068\n",
            "BLEU-3: 0.162821\n",
            "BLEU-4: 0.066602\n",
            "model6 :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-25c05c9da4e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' :'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-48-188a27c002bb>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, descriptions, photos, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mactual\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdesc_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdescriptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_descriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mphotos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdesc_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mactual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-6ecc1e6ca24a>\u001b[0m in \u001b[0;36mgenerate_descriptions\u001b[0;34m(model, tokenizer, photo, maxlen)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min_text\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1611\u001b[0m             \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'outputs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_outputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1614\u001b[0m     \u001b[0mall_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_predict_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    583\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m           \u001b[0mnumpy_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mto_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[1;32m    635\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m       expand_composites=expand_composites)\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    521\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     final_index, packed = _packed_nest_with_indices(structure, flat_sequence,\n\u001b[0;32m--> 523\u001b[0;31m                                                     0, is_seq, sequence_fn)\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_index\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_packed_nest_with_indices\u001b[0;34m(structure, flat, index, is_seq, sequence_fn)\u001b[0m\n\u001b[1;32m    482\u001b[0m   \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m   \u001b[0msequence_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_sequence_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       new_index, child = _packed_nest_with_indices(s, flat, index, is_seq,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_yield_value\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_yield_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_yield_sorted_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_yield_sorted_items\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    221\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m   \u001b[0;31m# namedtuples handled separately to avoid expensive namedtuple check.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m   \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHN6RzfAtHs7"
      },
      "source": [
        "\n",
        "dump(tokenizer, open('Image_Caption_Project/tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v9hUh7x8s_C"
      },
      "source": [
        "#image caption model for new input image not present in dataset\n",
        "def extract_feature(directory):\n",
        "  model = VGG16()\n",
        "  model = Model(inputs = model.input , outputs = model.layers[-2].output)\n",
        "  image = load_img(directory, target_size=(224, 224))\n",
        "  image = img_to_array(image)\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  image = preprocess_input(image)\n",
        "  feature = model.predict(image, verbose=0)\n",
        "  return feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkK4Nc8D_3gx"
      },
      "source": [
        "filename = 'Image_Caption_Project/tokenizer.pkl'\n",
        "tokenizer = pickle.load(open(filename , 'rb'))\n",
        "model = load_model('Image_Caption_Project/model_2.h5')\n",
        "photo = extract_feature('Image_Caption_Project/example2.jpg')\n",
        "maxlen = 34\n",
        "desc = generate_descriptions(model , tokenizer , photo , maxlen)\n",
        "words = desc.split()\n",
        "description = str()\n",
        "for w in words:\n",
        "  if w == 'startseq' or w == 'endseq':\n",
        "    continue\n",
        "  description += w + ' '"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVIXGKuzB7FZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ee53c4f-4656-42da-f69b-307166c22f54"
      },
      "source": [
        "print(description)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "two children are playing in the field \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6i_MctpCdxS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}